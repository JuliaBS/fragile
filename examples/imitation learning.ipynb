{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "from plangym.wrappers import FireResetEnv, FrameStack, MaxAndSkipEnv, NoopResetEnv\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "def wrap(env):\n",
    "    env = NoopResetEnv(env, noop_max=15, override=True)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = MaxAndSkipEnv(env, skip=3)\n",
    "    env = FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\n",
    "def show_video():\n",
    "    \n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env_video(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fragile.learning.imitation_atari.network import ConvolutionalNeuralNetwork, ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import atari_py\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from plangym import AtariEnvironment\n",
    "\n",
    "from fragile.core import DiscreteEnv, DiscreteUniform, GaussianDt\n",
    "from fragile.core.tree import HistoryTree\n",
    "from fragile.core.swarm import Swarm\n",
    "from fragile.distributed import ParallelEnv\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAME_SIZE, FRAME_SIZE, FRAMES_IN_OBSERVATION)\n",
    "MEMORY_SIZE = 90000\n",
    "EXPLORE_MEMORY_STEPS = 1\n",
    "\n",
    "\n",
    "class FragileRunner:\n",
    "    def __init__(self, game_name):\n",
    "\n",
    "        self.env = AtariEnvironment(\n",
    "            name=game_name,\n",
    "            clone_seeds=True,\n",
    "            wrappers=[wrap],\n",
    "        )\n",
    "\n",
    "        self.game_name = game_name\n",
    "        self.env_callable = lambda: ParallelEnv(lambda: DiscreteEnv(env=self.env), n_workers=8)\n",
    "        self.model_callable = lambda env:DiscreteUniform(env=self.env)\n",
    "        self.prune_tree = True\n",
    "        # A bigger number will increase the quality of the trajectories sampled.\n",
    "        self.n_walkers = 64\n",
    "        self.max_epochs = 400  # Increase to sample longer games.\n",
    "        self.reward_scale = 2  # Rewards are more important than diversity.\n",
    "        self.distance_scale = 1\n",
    "        self.minimize = False  # We want to get the maximum score possible.\n",
    "        self.swarm = swarm = Swarm(\n",
    "            model=self.model_callable,\n",
    "            env=self.env_callable,\n",
    "            tree=lambda: HistoryTree(names=[\"observs\", \"actions\", \"states\"], prune=True),\n",
    "            n_walkers=self.n_walkers,\n",
    "            max_epochs=self.max_epochs,\n",
    "            prune_tree=self.prune_tree,\n",
    "            reward_scale=self.reward_scale,\n",
    "            distance_scale=self.distance_scale,\n",
    "            minimize=self.minimize,\n",
    "            score_limit=600\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Creating fractal replay memory...\")\n",
    "        _ = self.swarm.run()\n",
    "        print(\"Max. fractal cum_rewards:\", self.swarm.best_reward)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FractalExplorationImitationLearning:\n",
    "\n",
    "    def __init__(self):\n",
    "        # We choose a game\n",
    "        game_name = \"SpaceInvaders\"\n",
    "\n",
    "        # Choose after how many runs we should stop\n",
    "        total_run_limit = 1\n",
    "        print(\"Selected game: \" + str(game_name))        \n",
    "        print(\"Total run limit: \" + str(total_run_limit))\n",
    "        \n",
    "        env_name = game_name + \"Deterministic-v4\"\n",
    "        env = wrap_env_video(wrap(gym.make(env_name)))\n",
    "        explorer = FragileRunner(env_name)\n",
    "        \n",
    "        # Game model\n",
    "        game_model = ModelTrainer(input_shape=INPUT_SHAPE, n_actions=env.action_space.n)\n",
    "\n",
    "        # model training\n",
    "        self._main_loop(env_name, explorer, game_model, total_run_limit)\n",
    "\n",
    "    def _main_loop(self, env_name, explorer, game_model, total_run_limit):\n",
    "        run = 0\n",
    "        while run < total_run_limit:\n",
    "            run += 1            \n",
    "            print(\"Training run:\", run)                         \n",
    "\n",
    "            # We explore the game space state using fragile framework  \n",
    "            for i in range(EXPLORE_MEMORY_STEPS):\n",
    "                explorer.run()\n",
    "                game_model.memorize(explorer.swarm)\n",
    "                print(\"model has %s memories\" % len(game_model.action_memory))\n",
    "\n",
    "            # Training a run                       \n",
    "            metrics = game_model.train(epochs=200, batch_size=32, verbose=0)\n",
    "            \n",
    "            \n",
    "            # Testing model\n",
    "            clear_output(True)\n",
    "            print(\"Model evaluation metrics:\\n %s\" % metrics)\n",
    "            score = game_model.evaluate(explorer.swarm)\n",
    "            \n",
    "            print(\"Neural Network score:\", score)\n",
    "            show_video()   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FractalExplorationImitationLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"SpaceInvaders\"\n",
    "\n",
    "# Choose after how many runs we should stop\n",
    "total_run_limit = 1\n",
    "print(\"Selected game: \" + str(game_name))        \n",
    "print(\"Total run limit: \" + str(total_run_limit))\n",
    "\n",
    "env_name = game_name + \"Deterministic-v4\"\n",
    "env = wrap_env_video(wrap(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = ModelTrainer(input_shape=INPUT_SHAPE, n_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
